{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reusable Code Chunks for ML Project App"
      ],
      "metadata": {
        "id": "fBNx4LrDYaYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Preparation"
      ],
      "metadata": {
        "id": "kMfH-WxEYWqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwx5XCCGCO2-",
        "outputId": "0ad89516-7f07-4858-b87a-e775d4cd386f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.5.1+cu124\n",
            "Uninstalling torch-2.5.1+cu124:\n",
            "  Would remove:\n",
            "    /usr/local/bin/convert-caffe2-to-onnx\n",
            "    /usr/local/bin/convert-onnx-to-caffe2\n",
            "    /usr/local/bin/torchfrtrace\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.11/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torch-2.5.1+cu124.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torchgen/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled torch-2.5.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu125"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESgRlq0dCUjI",
        "outputId": "b1e7f94d-20bb-4c37-8ee2-426a8bf39258"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu125\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n",
        "!pip install datasets\n",
        "!pip install wandb\n",
        "!pip install fastapi\n",
        "!pip install \"fastapi[standard]\"\n",
        "!pip install gradio\n",
        "!pip install streamlit\n",
        "!pip install uvicorn\n",
        "!pip install dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqazdLDeWdNf",
        "outputId": "a1479890-b6f9-4ea1-b8dc-fdcc128359fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.11)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: fastapi[standard] in /usr/local/lib/python3.11/dist-packages (0.115.11)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (0.46.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (4.12.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (0.0.7)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (0.28.1)\n",
            "Requirement already satisfied: jinja2>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (3.1.5)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (0.0.20)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (2.2.0)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (0.34.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]) (2.7.0)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]) (3.10)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (0.15.2)\n",
            "Requirement already satisfied: rich-toolkit>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (0.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.1.5->fastapi[standard]) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[standard]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[standard]) (2.27.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.12.0->uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (8.1.8)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (1.0.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (6.0.2)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (1.0.4)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (14.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->fastapi[standard]) (1.3.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (0.1.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.20.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.11)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.7.2)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.9.10)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.43.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.29.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: dotenv in /usr/local/lib/python3.11/dist-packages (0.9.9)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from dotenv) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "THbJwxUzpWDm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IN_COLAB = 'google.colab' in sys.modules"
      ],
      "metadata": {
        "id": "C7BdIiT93OYS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    DATASETS_PATH = '/content/drive/MyDrive/customer-support-tickets/datasets'\n",
        "    MODELS_PATH = '/content/drive/MyDrive/customer-support-tickets/models'\n",
        "else:\n",
        "    DATASETS_PATH = './customer-support-tickets/datasets'\n",
        "    MODELS_PATH = './customer-support-tickets/models'\n",
        "\n",
        "    load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "natTRPdS3RRr",
        "outputId": "c80259b9-fea1-4dcc-8e9f-571bab6a7dfa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backend"
      ],
      "metadata": {
        "id": "HP7aelgWYy7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File preprocessing.py"
      ],
      "metadata": {
        "id": "aiT5GVa-01_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from typing import List, Tuple, Dict, Any, Optional, Union\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "class EmailPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def fit(self, df: pd.DataFrame):\n",
        "        return self\n",
        "\n",
        "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        prep_steps = [\n",
        "            self.filter_language,\n",
        "            self.fill_missing_values,\n",
        "            self.attach_subject_body,\n",
        "            self.extract_features\n",
        "        ]\n",
        "\n",
        "        for step in prep_steps:\n",
        "            df_prep = step(df_prep)\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "    def filter_language(self, df: pd.DataFrame, lang: str = 'en') -> pd.DataFrame:\n",
        "        if 'language' not in df.columns:\n",
        "            df['language'] = df['text'].apply(lambda x: self.detect_language(x))\n",
        "\n",
        "        df_filtered = df[df['language'] == lang].copy()\n",
        "\n",
        "        return df_filtered\n",
        "\n",
        "    def fill_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_filled = df.copy()\n",
        "        df_filled['subject'] = df_filled['subject'].fillna('no subject')\n",
        "        df_filled['body'] = df_filled['body'].fillna('no body')\n",
        "\n",
        "        return df_filled\n",
        "\n",
        "    def extract_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_featured = df.copy()\n",
        "\n",
        "        df_featured['subject_length'] = df_featured['subject'].apply(lambda x: len(x))\n",
        "        df_featured['body_length'] = df_featured['body'].apply(lambda x: len(x))\n",
        "\n",
        "        return df_featured\n",
        "\n",
        "    def attach_subject_body(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_attached = df.copy()\n",
        "        df_attached['text'] = df_attached.apply(\n",
        "            lambda row: f\"Subject: {row['subject']}\\nBody: {row['body']}\",\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        return df_attached\n",
        "\n",
        "    def detect_language(self, text: str, lang: str = 'en') -> str:\n",
        "        try:\n",
        "            return langdetect.detect(text)\n",
        "        except langdetect.lang_detect_exception.LangDetectException:\n",
        "            return 'unknown'\n",
        "\n",
        "class ResamplingPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        label_columns: str | List[str],\n",
        "        resample_mode: str = 'undersample',\n",
        "        random_state: int = 42,\n",
        "        n_samples_each_category: int = None\n",
        "    ) -> None:\n",
        "        if type(label_columns) == str:\n",
        "            self.label_columns = [label_columns]\n",
        "        else:\n",
        "            self.label_columns = label_columns\n",
        "\n",
        "        self.random_state = random_state\n",
        "        self.resample_mode = resample_mode\n",
        "        self.n_samples_each_category = n_samples_each_category\n",
        "\n",
        "    def fit(self, df: pd.DataFrame):\n",
        "        return self\n",
        "\n",
        "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        prep_steps = [\n",
        "            self.resample\n",
        "        ]\n",
        "\n",
        "        for step in prep_steps:\n",
        "            df_prep = step(df_prep)\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "    def resample(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        if self.resample_mode == 'undersample':\n",
        "            df_prep = self.undersample_examples(df=df_prep)\n",
        "        elif self.resample_mode == 'oversample':\n",
        "            df_prep = self.oversample_examples(df=df_prep)\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "    def undersample_examples(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Undersamples a DataFrame based on a subset of columns to create a balanced dataset.\n",
        "\n",
        "        Args:\n",
        "            columns (List[str]): The columns to group the DataFrame by for undersampling.\n",
        "            n_samples_each_category (int, optional): The number of samples to keep from each category. If None, all samples from each category will be kept. Defaults to None.\n",
        "            random_state (int, optional): The random state for reproducibility. Defaults to 42.\n",
        "        \"\"\"\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        if self.n_samples_each_category is None:\n",
        "            n_samples = df_prep.groupby(self.label_columns).size().min()\n",
        "        else:\n",
        "            n_samples = self.n_samples_each_category\n",
        "\n",
        "        undersampled_dfs = df_prep.groupby(self.label_columns).sample(n=n_samples, replace=False, random_state=self.random_state)\n",
        "\n",
        "        return undersampled_dfs\n",
        "\n",
        "    def oversample_examples(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Oversamples a DataFrame based on a subset of columns to create a balanced dataset.\n",
        "\n",
        "        Args:\n",
        "            columns (List[str]): The columns to group the DataFrame by for oversampling.\n",
        "            random_state (int, optional): The random state for reproducibility. Defaults to 42.\n",
        "        \"\"\"\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        max_samples = df_prep.groupby(self.label_columns).size().max()\n",
        "        oversampled_dfs = []\n",
        "\n",
        "        for _, group_df in df_prep.groupby(self.label_columns):\n",
        "            n_samples = max_samples - len(group_df)\n",
        "\n",
        "            oversampled_dfs.append(group_df.sample(n=n_samples, replace=True, random_state=self.random_state))\n",
        "\n",
        "        oversampled_dfs = pd.concat(oversampled_dfs)\n",
        "\n",
        "        return oversampled_dfs\n",
        "\n",
        "class SplitterPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        split_mode: str = 'train_val_test',\n",
        "        retrieve: str = 'all',\n",
        "        test_size: float = 0.2,\n",
        "        val_size: float = 0.2,\n",
        "        random_state: int = 42\n",
        "    ) -> None:\n",
        "        self.split_mode = split_mode\n",
        "        self.retrieve = retrieve\n",
        "\n",
        "        self.test_size = test_size\n",
        "        self.val_size = val_size\n",
        "\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, df: pd.DataFrame):\n",
        "        return self\n",
        "\n",
        "    def transform(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        if self.split_mode == 'train_val_test':\n",
        "            df_train, df_val, df_test = self.split_train_val_test(df_prep)\n",
        "        elif self.split_mode == 'train_test':\n",
        "            df_train, df_test = self.split_train_test(df_prep)\n",
        "        elif self.split_mode == 'train_val':\n",
        "            df_train, df_val = self.split_train_val(df_prep)\n",
        "        else:\n",
        "            raise ValueError(f'Invalid value for split_mode: {self.split_mode}')\n",
        "\n",
        "        if self.retrieve == 'all':\n",
        "            return df_train, df_val, df_test\n",
        "        elif self.retrieve == 'train':\n",
        "            return df_train\n",
        "        elif self.retrieve == 'val':\n",
        "            return df_val\n",
        "        elif self.retrieve == 'test':\n",
        "            return df_test\n",
        "        else:\n",
        "            raise ValueError(f'Invalid value for retrieve: {self.retrieve}')\n",
        "\n",
        "    def split_train_val_test(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "        df_train_val, df_test = train_test_split(df, test_size=self.test_size, random_state=self.random_state)\n",
        "        df_train, df_val = train_test_split(df_train_val, test_size=self.val_size, random_state=self.random_state)\n",
        "\n",
        "        return df_train, df_val, df_test\n",
        "\n",
        "    def split_train_test(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        df_train, df_test = train_test_split(df, test_size=self.test_size, random_state=self.random_state)\n",
        "\n",
        "        return df_train, df_test\n",
        "\n",
        "    def split_train_val(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        df_train, df_val = train_test_split(df, test_size=self.val_size, random_state=self.random_state)\n",
        "\n",
        "        return df_train, df_val\n",
        "\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        stopwords: List[str] = None,\n",
        "        flg_stemm: bool = False,\n",
        "        flg_lemm: bool = False,\n",
        "        flg_stopwords: bool = True,\n",
        "        flg_punctuation: bool = True,\n",
        "        flg_numbers: bool = True\n",
        "    ) -> None:\n",
        "        self.flg_stemm = flg_stemm\n",
        "        self.flg_lemm = flg_lemm\n",
        "        self.flg_stopwords = flg_stopwords\n",
        "        self.flg_punctuation = flg_punctuation\n",
        "        self.flg_numbers = flg_numbers\n",
        "\n",
        "        self.load_stopwords(stopwords=stopwords)\n",
        "\n",
        "    def fit(self, df: pd.DataFrame):\n",
        "        return self\n",
        "\n",
        "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_prep = df.copy()\n",
        "        df_prep['text'] = df_prep['text'].apply(lambda x: self.preprocess_text(x))\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "    def load_stopwords(\n",
        "        self,\n",
        "        stopwords: List[str] = None,\n",
        "        lang: str = 'english',\n",
        "        source: str = 'nltk',\n",
        "        file_path: str = ''\n",
        "    ):\n",
        "        if stopwords is not None:\n",
        "            self.stopwords = stopwords\n",
        "        elif source == 'nltk':\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('wordnet')\n",
        "\n",
        "            self.stopwords = nltk.corpus.stopwords.words(lang)\n",
        "        elif source == 'spacy':\n",
        "            spacy.load('en_core_web_sm')\n",
        "\n",
        "            self.stopwords = spacy.load(lang).Defaults.stop_words\n",
        "        elif source == 'file':\n",
        "            try:\n",
        "                with open(file_path, 'r') as f:\n",
        "                    self.stopwords = f.read().splitlines()\n",
        "            except FileNotFoundError:\n",
        "                print(f'File not found: {file_path}')\n",
        "                self.stopwords = []\n",
        "        else:\n",
        "            self.stopwords = []\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        import unicodedata\n",
        "\n",
        "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        text = text.lower()\n",
        "\n",
        "        if self.flg_punctuation:\n",
        "            text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        if self.flg_numbers:\n",
        "            text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "        text = re.sub(r'\\s{2,}', ' ', text)\n",
        "\n",
        "        if self.flg_stemm:\n",
        "            ps = nltk.stem.porter.PorterStemmer()\n",
        "            text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "\n",
        "        if self.flg_lemm:\n",
        "            lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "            text = ' '.join([lem.lemmatize(word) for word in text.split()])\n",
        "\n",
        "        if self.flg_stopwords:\n",
        "            stopwords = nltk.corpus.stopwords.words('english')\n",
        "            text = ' '.join([word for word in text.split() if word not in stopwords])\n",
        "\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "class LabelPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, label_column_name: str, encoder_mode: str = 'label') -> None:\n",
        "        self.label_column_name = label_column_name\n",
        "        self.encoder_mode = encoder_mode\n",
        "\n",
        "        if self.encoder_mode == 'label':\n",
        "            self.encoder = LabelEncoder()\n",
        "        elif self.encoder_mode == 'onehot':\n",
        "            self.encoder = OneHotEncoder()\n",
        "\n",
        "    def fit(self, df: pd.DataFrame):\n",
        "        return self\n",
        "\n",
        "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        if self.label_column_name in df_prep.columns:\n",
        "            prep_steps = [\n",
        "                self.set_label,\n",
        "                self.encode\n",
        "            ]\n",
        "\n",
        "            for step in prep_steps:\n",
        "                df_prep = step(df_prep)\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "    def set_label(self, df: pd.DataFrame):\n",
        "        df_prep = df.copy()\n",
        "        df_prep['label'] = df_prep[self.label_column_name]\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "    def encode(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        df_prep['label'] = self.encoder.fit_transform(df_prep['label'])\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "    def decode(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        df_prep['label'] = self.encoder.inverse_transform(df_prep['label'])\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "class VectorizerPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        from_file: bool = False,\n",
        "        file_path: str = '',\n",
        "        vectorizer_mode: str = 'TfIdfVectorizer'\n",
        "    ) -> None:\n",
        "        self.from_file = from_file\n",
        "        self.file_path = file_path\n",
        "        self.vectorizer_mode = vectorizer_mode\n",
        "\n",
        "        if from_file:\n",
        "            try:\n",
        "                self.vectorizer = self.load_vectorizer(file_path)\n",
        "                self.vectorizer_mode = self.vectorizer.__class__.__name__\n",
        "            except FileNotFoundError:\n",
        "                self.vectorizer = TfidfVectorizer(ngram_range=(1, 1), smooth_idf=True, use_idf=True)\n",
        "                self.vectorizer_mode = self.vectorizer.__class__.__name__\n",
        "\n",
        "        elif self.vectorizer_mode == 'TfIdfVectorizer':\n",
        "            self.vectorizer = TfidfVectorizer(ngram_range=(1, 1), smooth_idf=True, use_idf=True)\n",
        "        elif self.vectorizer_mode == 'CountVectorizer':\n",
        "            self.vectorizer = CountVectorizer()\n",
        "        else:\n",
        "            raise ValueError(f'Invalid value for vectorizer_mode: {self.vectorizer_mode}')\n",
        "\n",
        "    def fit(self, df: pd.DataFrame):\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        if self.vectorizer:\n",
        "            self.vectorizer.fit(df_prep['text'])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        if self.vectorizer:\n",
        "            # Get the sparse matrix from the vectorizer\n",
        "            feature_matrix = self.vectorizer.transform(df_prep['text'])\n",
        "            # Convert sparse matrix to dense array and create a DataFrame\n",
        "            # feature_df = pd.DataFrame(feature_matrix.toarray())\n",
        "            feature_df = pd.DataFrame(feature_matrix.toarray(), columns=self.vectorizer.get_feature_names_out())\n",
        "\n",
        "            for col in df_prep.columns:\n",
        "                if col in feature_df.columns:\n",
        "                    df_prep.rename(columns={col: f'{col}_original'}, inplace=True)\n",
        "\n",
        "            # Concatenate feature DataFrame with original DataFrame\n",
        "            df_prep = pd.concat(\n",
        "                [df_prep.reset_index(drop=True), feature_df],\n",
        "                axis=1\n",
        "            ) # Concatenate feature columns\n",
        "\n",
        "        else:\n",
        "            df_prep['features'] = df_prep['text']\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "    def get_feature_names(self):\n",
        "        return self.vectorizer.get_feature_names_out()\n",
        "\n",
        "    def get_vocabulary(self):\n",
        "        return self.vectorizer.vocabulary_\n",
        "\n",
        "    def load_vectorizer(self, file_path: str):\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                vectorizer = pickle.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f'File not found: {file_path}')\n",
        "            vectorizer = None\n",
        "\n",
        "        return vectorizer\n",
        "\n",
        "    def save_vectorizer(self, file_path: str):\n",
        "        try:\n",
        "            with open(file_path, 'wb') as f:\n",
        "                pickle.dump(self.vectorizer, f)\n",
        "        except FileNotFoundError:\n",
        "            print(f'File not found: {file_path}')\n",
        "            vectorizer = None"
      ],
      "metadata": {
        "id": "YV7CxA8SuMyF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File models.py"
      ],
      "metadata": {
        "id": "mn9GQtWk1Jeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "from typing import List, Tuple, Dict, Any, Optional, Union\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_fscore_support, roc_auc_score, roc_curve\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "import wandb\n",
        "\n",
        "class BaselineModel(BaseEstimator, ClassifierMixin):\n",
        "    model_mapping = {\n",
        "        'MultinomialNB': MultinomialNB,\n",
        "        'BernoulliNB': BernoulliNB,\n",
        "        'GaussianNB': GaussianNB,\n",
        "        'LogisticRegression': LogisticRegression,\n",
        "        'KNeighborsClassifier': KNeighborsClassifier,\n",
        "        'SVC': SVC,\n",
        "        'DecisionTreeClassifier': DecisionTreeClassifier,\n",
        "        'RandomForestClassifier': RandomForestClassifier,\n",
        "        'GradientBoostingClassifier': GradientBoostingClassifier\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        from_file: bool = False,\n",
        "        file_path: str = '',\n",
        "        model: Union[str, BaseEstimator] = 'LogisticRegression',\n",
        "        input_column_names_numeric: bool = False,\n",
        "        input_column_names: List[str] | str = 'text',\n",
        "        output_column_name: str = 'label',\n",
        "        device: str = 'cpu',\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        if from_file:\n",
        "            try:\n",
        "                self.model = self.load_model(file_path)\n",
        "                self.model_name = self.model.__class__.__name__\n",
        "            except FileNotFoundError:\n",
        "                self.model = self.model_mapping[model](**kwargs)\n",
        "                self.model_name = model\n",
        "        elif type(model) == str:\n",
        "            self.model = self.model_mapping[model](**kwargs)\n",
        "            self.model_name = model\n",
        "        else:\n",
        "            self.model = model\n",
        "            self.model_name = model.__class__.__name__\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "        self.input_column_names_numeric = input_column_names_numeric # Store the argument as an attribute\n",
        "\n",
        "        if input_column_names_numeric:\n",
        "            self.input_column_names = None\n",
        "        elif type(input_column_names) == str:\n",
        "            self.input_column_names = [input_column_names]\n",
        "        else:\n",
        "            self.input_column_names = input_column_names\n",
        "\n",
        "        self.output_column_name = output_column_name\n",
        "\n",
        "    def get_numeric_columns(self, df: pd.DataFrame):\n",
        "        return [col for col in df.columns if str(col).isnumeric()]\n",
        "\n",
        "    def fit(self, df: pd.DataFrame, y: pd.Series = None):\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        if self.input_column_names_numeric and self.input_column_names is None:\n",
        "            self.input_column_names = self.get_numeric_columns(df_prep)\n",
        "\n",
        "        X = df_prep[self.input_column_names]\n",
        "        y = y if y is not None else df_prep[self.output_column_name]\n",
        "\n",
        "        self.model.fit(X, y)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, df: pd.DataFrame):\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        if self.input_column_names_numeric and self.input_column_names is None:\n",
        "            self.input_column_names = self.get_numeric_columns(df_prep)\n",
        "\n",
        "        X = df_prep[self.input_column_names]\n",
        "\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def score(self, df: pd.DataFrame, y: pd.Series = None):\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        if self.input_column_names_numeric and self.input_column_names is None:\n",
        "            self.input_column_names = self.get_numeric_columns(df_prep)\n",
        "\n",
        "        X = df_prep[self.input_column_names]\n",
        "        y = y if y is not None else df_prep[self.output_column_name]\n",
        "\n",
        "        return self.model.score(X, y)\n",
        "\n",
        "    def predict_proba(self, df: pd.DataFrame):\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        if self.input_column_names_numeric and self.input_column_names is None:\n",
        "            self.input_column_names = self.get_numeric_columns(df_prep)\n",
        "\n",
        "        X = df_prep[self.input_column_names]\n",
        "\n",
        "        return self.model.predict_proba(X)\n",
        "\n",
        "    def get_params(self, deep: bool = True):\n",
        "        return self.model.get_params(deep)\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        return self.model.set_params(**params)\n",
        "\n",
        "    def load_model(self, file_path: str):\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                model = pickle.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f'File not found: {file_path}')\n",
        "            model = None\n",
        "\n",
        "        return model\n",
        "\n",
        "    def save_model(self, file_path: str):\n",
        "        try:\n",
        "            with open(file_path, 'wb') as f:\n",
        "                pickle.dump(self.model, f)\n",
        "        except FileNotFoundError:\n",
        "            print(f'File not found: {file_path}')\n",
        "            model = None\n",
        "\n",
        "class TransformerModel(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        load_path: str = 'bert-base-uncased',\n",
        "        save_path: str = './saved_models',\n",
        "        run_name: str = 'bert-base-uncased',\n",
        "        input_column_name: str = 'text',\n",
        "        output_column_name: str = 'label',\n",
        "        num_labels: int = 2,\n",
        "        device: str = 'cpu',\n",
        "        output_dir: str = './results',\n",
        "        logging_dir: str = './logs',\n",
        "        epochs: int=3,\n",
        "        per_device_train_batch_size: int = 4,\n",
        "        per_device_eval_batch_size: int = 4,\n",
        "        learning_rate: float = 2e-5,\n",
        "        weight_decay: float = 0.01\n",
        "    ) -> None:\n",
        "        self.load_path = load_path\n",
        "        self.save_path = save_path\n",
        "\n",
        "        self.num_labels = num_labels\n",
        "        self.device = device\n",
        "\n",
        "        self.load_models()\n",
        "\n",
        "        self.run_name = run_name\n",
        "\n",
        "        self.input_column_name = input_column_name\n",
        "        self.output_column_name = output_column_name\n",
        "\n",
        "        self.output_dir = output_dir\n",
        "        self.logging_dir = logging_dir\n",
        "\n",
        "        self.epochs = epochs\n",
        "        self.per_device_train_batch_size = per_device_train_batch_size\n",
        "        self.per_device_eval_batch_size = per_device_eval_batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "    def load_models(self):\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.load_path, use_fast=True)\n",
        "        except Exception:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.load_path, use_fast=False)\n",
        "\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(self.load_path, num_labels=self.num_labels).to(self.device)\n",
        "\n",
        "    def save_models(self):\n",
        "        self.trainer.save_model(self.save_path)\n",
        "        self.tokenizer.save_pretrained(self.save_path)\n",
        "        self.model.save_pretrained(self.save_path)\n",
        "\n",
        "    def tokenize(self, texts: Dict[str, list], max_length: int = 128):\n",
        "        return self.tokenizer(texts, padding=True, truncation=True, max_length=max_length)\n",
        "\n",
        "    def compute_intermediate_metrics(self, eval_pred: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
        "        metrics = {}\n",
        "\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "        metrics['accuracy'] = accuracy_score(y_true=labels, y_pred=predictions)\n",
        "        metrics['precision'], metrics['recall'], metrics['f1'], _ = precision_recall_fscore_support(y_true=labels, y_pred=predictions, average=\"weighted\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def fit(self, df_train: pd.DataFrame, df_val: pd.DataFrame, y: pd.Series = None):\n",
        "        train_dataset = Dataset.from_pandas(pd.DataFrame(\n",
        "            {\n",
        "                'text': df_train[self.input_column_name],\n",
        "                'label': df_train[self.output_column_name]\n",
        "            }\n",
        "        ))\n",
        "        val_dataset = Dataset.from_pandas(pd.DataFrame(\n",
        "            {\n",
        "                'text': df_val[self.input_column_name],\n",
        "                'label': df_val[self.output_column_name]\n",
        "            }\n",
        "        ))\n",
        "\n",
        "        tokenized_train_dataset = train_dataset.map(self.tokenize, batched=True)\n",
        "        tokenized_val_dataset = val_dataset.map(self.tokenize, batched=True)\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            run_name=self.run_name,\n",
        "\n",
        "            output_dir=self.output_dir,\n",
        "            logging_dir=self.logging_dir,\n",
        "\n",
        "            report_to=\"wandb\",\n",
        "\n",
        "            num_train_epochs=self.epochs,\n",
        "            per_device_train_batch_size=self.per_device_train_batch_size,\n",
        "            per_device_eval_batch_size=self.per_device_eval_batch_size,\n",
        "            learning_rate=self.learning_rate,\n",
        "            weight_decay=self.weight_decay,\n",
        "\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "\n",
        "            load_best_model_at_end=True\n",
        "        )\n",
        "\n",
        "        # Create Trainer instance\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_train_dataset,\n",
        "            eval_dataset=tokenized_val_dataset,\n",
        "            processing_class=self.tokenizer,\n",
        "            compute_metrics=self.compute_intermediate_metrics\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        trainer.train()\n",
        "\n",
        "        # Evaluate the model\n",
        "        eval_results = trainer.evaluate()\n",
        "\n",
        "        self.trainer = trainer\n",
        "\n",
        "        # [optional] Finish the wandb run, necessary in notebooks\n",
        "        wandb.finish()\n",
        "\n",
        "    def compute_predictions_details(self, df: pd.DataFrame):\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        # Tokenize and prepare input\n",
        "        df_prep['embeddings'] = df_prep[self.input_column_name].apply(\n",
        "            lambda text: self.tokenizer(\n",
        "                text=text,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(self.device)\n",
        "        )\n",
        "\n",
        "        # Ensure model is in evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Run inference\n",
        "        with torch.no_grad():\n",
        "            df_prep['logits'] = df_prep['embeddings'].apply(lambda inputs: self.model(**inputs).logits)\n",
        "            df_prep['predictions'] = df_prep['logits'].apply(lambda logits: torch.argmax(logits, dim=-1).item())\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "\n",
        "    def predict(self, df: pd.DataFrame):\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        df_prep = self.compute_predictions_details(df_prep)\n",
        "\n",
        "        return df_prep['predictions']\n",
        "\n",
        "    def predict_proba(self, df: pd.DataFrame):\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        df_prep = self.compute_predictions_details(df_prep)\n",
        "\n",
        "        return df_prep['logits']"
      ],
      "metadata": {
        "id": "e6a5PdPP1Ju9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "501497a5-52e8-4baa-b484-ea37fa6c303f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-86a2b399c316>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File evaluation.py"
      ],
      "metadata": {
        "id": "SVNIrTHheoK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_fscore_support, roc_auc_score, roc_curve\n",
        "\n",
        "class MetricsEvaluator(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, label_column_name: str = 'label') -> None:\n",
        "        self.label_column_name = label_column_name\n",
        "\n",
        "    def fit(self, df: pd.DataFrame):\n",
        "        return self\n",
        "\n",
        "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        df_prep = self.calculate_metrics(df_prep)\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "    def calculate_metrics(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        metrics = {}\n",
        "\n",
        "        metrics['confusion_matrix'] = confusion_matrix(df[self.label_column_name], df['prediction'])\n",
        "        metrics['classification_report'] = classification_report(df[self.label_column_name], df['prediction'])\n",
        "\n",
        "        metrics['accuracy'] = accuracy_score(df[self.label_column_name], df['prediction'])\n",
        "        metrics['precision'], metrics['recall'], metrics['f1_score'], _ = precision_recall_fscore_support(df[self.label_column_name], df['prediction'], average='weighted')\n",
        "\n",
        "        return metrics"
      ],
      "metadata": {
        "id": "cRguDj8ueb1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File pipelines.py"
      ],
      "metadata": {
        "id": "Avw7qroh1Tb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "from typing import List, Tuple, Dict, Any, Optional, Union\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_fscore_support, roc_auc_score, roc_curve\n",
        "\n",
        "# from src.backend.evaluation import MetricsEvaluator\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "class PipelineModules(Pipeline):\n",
        "    def __init__(\n",
        "        self,\n",
        "        from_file: bool = False,\n",
        "        file_path: str = '',\n",
        "        steps: Tuple[str, Any] = None,\n",
        "        memory: str = None,\n",
        "        device: str = 'cpu',\n",
        "        verbose: bool = False\n",
        "    ) -> None:\n",
        "        if from_file:\n",
        "            try:\n",
        "                self.steps = self.load_pipeline(file_path)\n",
        "            except FileNotFoundError:\n",
        "                super().__init__(steps, memory=memory, verbose=verbose)\n",
        "                self.steps = steps\n",
        "        else:\n",
        "            super().__init__(steps, memory=memory, verbose=verbose)\n",
        "            self.steps = steps\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        if self.steps and len(self.steps) > 0:\n",
        "            print(self.steps)\n",
        "            self.pipeline_transformers = self.get_pipeline_transformers()\n",
        "            self.classifier = self.get_classifier()\n",
        "\n",
        "    def get_pipeline_transformers(self):\n",
        "        transformers = []\n",
        "\n",
        "        for step_name, step in self.steps:\n",
        "            if issubclass(type(step), TransformerMixin):\n",
        "                transformers.append((step_name, step))\n",
        "\n",
        "        transformers = Pipeline(transformers)\n",
        "\n",
        "        return transformers\n",
        "\n",
        "    def get_classifier(self):\n",
        "        classifiers = []\n",
        "\n",
        "        for step_name, step in self.steps:\n",
        "            if issubclass(type(step), ClassifierMixin):\n",
        "                classifiers.append((step_name, step))\n",
        "                # if isinstance(type(step), TransformerModel):\n",
        "                #     step.device = self.device\n",
        "                #     model = TransformerModel(**step.kwargs)\n",
        "                #     processed_step = processed_step.load_models()\n",
        "                #     classifiers.append((step_name, processed_step))\n",
        "                # else:\n",
        "                #     classifiers.append((step_name, step))\n",
        "\n",
        "        if len(classifiers) == 0:\n",
        "            raise ValueError('No classifier found in the pipeline')\n",
        "        else:\n",
        "            classifier = classifiers[0][1]\n",
        "\n",
        "        return classifier\n",
        "\n",
        "    def get_feature_names_out(self, input_features=None):\n",
        "        return super().get_feature_names_out(input_features)\n",
        "\n",
        "    def fit(self, df: pd.DataFrame, y: pd.Series = None):\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        df_prep = self.pipeline_transformers.fit_transform(df_prep)\n",
        "\n",
        "        features_columns = list(self.pipeline_transformers.named_steps['vectorizer'].get_feature_names())\n",
        "\n",
        "        self.classifier.input_column_names = features_columns\n",
        "\n",
        "        self.classifier.fit(df_prep)\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "    def transform(self, df: pd.DataFrame):\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        df_prep = self.pipeline_transformers.transform(df_prep)\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "    def fit_transform(self, df: pd.DataFrame, y: pd.Series = None):\n",
        "        raise NotImplementedError('PipelineModules does not support fit_transform method')\n",
        "\n",
        "    def predict(self, df: pd.DataFrame):\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        df_prep['prediction'] = self.classifier.predict(df_prep)\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "    def predict_proba(self, df: pd.DataFrame):\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        # Get probabilities for each class\n",
        "        probabilities = self.classifier.predict_proba(df_prep)\n",
        "\n",
        "        # Assuming probabilities is a 2D array, get the probabilities of the predicted class\n",
        "        predicted_class_probs = probabilities[np.arange(probabilities.shape[0]), self.classifier.predict(df_prep)]\n",
        "\n",
        "        # Create a new column for the probabilities of the predicted class\n",
        "        df_prep['prediction_proba'] = predicted_class_probs\n",
        "\n",
        "        return df_prep\n",
        "\n",
        "    def evaluate(self, df: pd.DataFrame):\n",
        "        df_prep = df.copy()\n",
        "\n",
        "        evaluator = MetricsEvaluator()\n",
        "\n",
        "        metrics = evaluator.fit_transform(df_prep)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def load_pipeline(self, file_path: str):\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                pipeline = pickle.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f'File not found: {file_path}')\n",
        "            pipeline = None\n",
        "\n",
        "        return pipeline\n",
        "\n",
        "    def save_pipeline(self, file_path: str):\n",
        "        try:\n",
        "            with open(file_path, 'wb') as f:\n",
        "                pickle.dump(self, f)\n",
        "        except FileNotFoundError:\n",
        "            print(f'File not found: {file_path}')\n",
        "            pipeline = None"
      ],
      "metadata": {
        "id": "RAOtBEEC1Trf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "1q-k53A3okoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File pydantic_models.py"
      ],
      "metadata": {
        "id": "2YJ87Qd9ZFMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class EmailInput(BaseModel):\n",
        "    subject: str\n",
        "    body: str\n",
        "    model_choice: str  # e.g., \"nb\", \"lr\", \"distilbert\", \"bert\"\n",
        "\n",
        "class PredictionResponse(BaseModel):\n",
        "    queue: str\n",
        "    priority: str\n",
        "    details: dict"
      ],
      "metadata": {
        "id": "92zMT1bBogQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File labels.py"
      ],
      "metadata": {
        "id": "uVsniZzzZLIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_queue_values = [\n",
        "    'Customer Service',\n",
        "    'Technical Support',\n",
        "    'IT Support',\n",
        "    'Product Support',\n",
        "    'Billing and Payments',\n",
        "    'Service Outages and Maintenance',\n",
        "    'Human Resources',\n",
        "    'Returns and Exchanges',\n",
        "    'Sales and Pre-Sales',\n",
        "    'General Inquiry'\n",
        "]\n",
        "\n",
        "label_priority_values = [\n",
        "    'Medium',\n",
        "    'High',\n",
        "    'Low'\n",
        "]"
      ],
      "metadata": {
        "id": "qbNnEapNZN0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File devices.py"
      ],
      "metadata": {
        "id": "Ils76PdvZOtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def get_available_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return 'cuda'\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return 'mps'\n",
        "    else:\n",
        "        return 'cpu'"
      ],
      "metadata": {
        "id": "4k37lsZCZQ-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API"
      ],
      "metadata": {
        "id": "l3JSC_qNZV16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File main.py"
      ],
      "metadata": {
        "id": "Y5f_srSpoYHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"NB_PIPELINE_QUEUE_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/baseline/nb_pipeline_queue.pkl\"\n",
        "os.environ[\"NB_PIPELINE_PRIORITY_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/baseline/nb_pipeline_priority.pkl\"\n",
        "os.environ[\"LR_PIPELINE_QUEUE_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/baseline/lr_pipeline_queue.pkl\"\n",
        "os.environ[\"LR_PIPELINE_PRIORITY_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/baseline/lr_pipeline_priority.pkl\"\n",
        "os.environ[\"BERT_PIPELINE_QUEUE_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/transformers/bert-base-uncased_on_queue_for_3_epochs\"\n",
        "os.environ[\"BERT_PIPELINE_PRIORITY_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/transformers/bert-base-uncased_on_priority_for_3_epochs\"\n",
        "\n",
        "os.environ[\"API_URL\"] = \"http://localhost:8000/predict\"\n",
        "\n",
        "os.environ[\"VECTORIZER_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/baseline/vectorizer.pkl\"\n",
        "os.environ[\"NB_MODEL_QUEUE_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/baseline/nb_model.pkl\"\n",
        "os.environ[\"NB_MODEL_PRIORITY_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/baseline/nb_model.pkl\"\n",
        "os.environ[\"LR_MODEL_QUEUE_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/baseline/lr_model.pkl\"\n",
        "os.environ[\"LR_MODEL_PRIORITY_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/baseline/lr_model.pkl\"\n",
        "os.environ[\"BERT_TRANSFORMERS_MODEL_QUEUE_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/transformers/bert-base-uncased_on_queue_for_3_epochs\"\n",
        "os.environ[\"BERT_TRANSFORMERS_MODEL_PRIORITY_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/transformers/bert-base-uncased_on_priority_for_3_epochs\"\n",
        "os.environ[\"DISTILBERT_TRANSFORMERS_MODEL_QUEUE_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/transformers/distilbert-base-uncased_on_queue_for_3_epochs\"\n",
        "os.environ[\"DISTILBERT_TRANSFORMERS_MODEL_PRIORITY_PATH\"] = \"/content/drive/MyDrive/customer-support-tickets/models/transformers/distilbert-base-uncased_on_priority_for_3_epochs\"\n",
        "\n",
        "os.environ[\"LOAD_MODE\"] = 'model'\n",
        "os.environ[\"FIT_FLG\"] = 'True'"
      ],
      "metadata": {
        "id": "U6E5kZOjAK2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from fastapi import FastAPI, HTTPException\n",
        "import uvicorn\n",
        "\n",
        "# from src.backend.evaluation import MetricsEvaluator\n",
        "# from src.backend.preprocessing import SplitterPreprocessor, EmailPreprocessor, ResamplingPreprocessor, TextPreprocessor, LabelPreprocessor, VectorizerPreprocessor\n",
        "# from src.backend.models import BaselineModel, TransformerModel\n",
        "# from src.backend.pipelines import PipelineModules\n",
        "# from src.utils.pydantic_models import EmailInput, PredictionResponse\n",
        "# from src.utils.labels import label_queue_values, label_priority_values\n",
        "# from src.utils.devices import get_available_device\n",
        "# from datasets.data_examples import email_examples\n",
        "from data_examples import email_examples\n",
        "\n",
        "import wandb\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "\n",
        "wandb.login()\n",
        "load_dotenv()\n",
        "\n",
        "data_df = pd.DataFrame(email_examples)\n",
        "\n",
        "splitter = SplitterPreprocessor(retrieve='all')\n",
        "\n",
        "data_prep_train_df, data_prep_val_df, data_prep_text_df = splitter.fit_transform(data_df)\n",
        "\n",
        "\n",
        "app = FastAPI(title=\"Customer IT Support Prediction API\")\n",
        "\n",
        "\n",
        "def start():\n",
        "    \"\"\"Launched with `poetry run start` at root level\"\"\"\n",
        "    uvicorn.run(\"my_package.main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
        "def predict(email: EmailInput):\n",
        "    LOAD_MODE = os.getenv('LOAD_MODE')\n",
        "    FIT_FLG = eval(os.getenv('FIT_FLG'))\n",
        "\n",
        "    # Preprocess the input text\n",
        "    dict_email = {\n",
        "        'language': 'en',\n",
        "        'subject': email.subject,\n",
        "        'body': email.body\n",
        "    }\n",
        "    df_email = pd.DataFrame(dict_email, index=[0])\n",
        "\n",
        "    if email.model_choice.lower() in [\"nb\", \"lr\"]:\n",
        "        if LOAD_MODE == 'model':\n",
        "            vectorizer_path = os.getenv('VECTORIZER_PATH')\n",
        "            vectorizer = VectorizerPreprocessor(from_file=True, file_path=vectorizer_path)\n",
        "\n",
        "        if email.model_choice.lower() == 'nb':\n",
        "            if LOAD_MODE == 'pipeline':\n",
        "                nb_pipeline_queue_path = os.getenv('NB_PIPELINE_QUEUE_PATH')\n",
        "                nb_pipeline_priority_path = os.getenv('NB_PIPELINE_PRIORITY_PATH')\n",
        "\n",
        "                pipeline_queue = PipelineModules(\n",
        "                    from_file=True,\n",
        "                    file_path=nb_pipeline_queue_path\n",
        "                )\n",
        "\n",
        "                pipeline_priority = PipelineModules(\n",
        "                    from_file=True,\n",
        "                    file_path=nb_pipeline_priority_path\n",
        "                )\n",
        "            elif LOAD_MODE == 'model':\n",
        "                nb_model_queue_path = os.getenv('NB_MODEL_QUEUE_PATH')\n",
        "                nb_model_priority_path = os.getenv('NB_MODEL_PRIORITY_PATH')\n",
        "\n",
        "                model_queue = BaselineModel(from_file=True, file_path=nb_model_queue_path)\n",
        "                model_priority = BaselineModel(from_file=True, file_path=nb_model_priority_path)\n",
        "\n",
        "                pipeline_queue = PipelineModules(steps=[\n",
        "                    ('email_preprocessor', EmailPreprocessor()),\n",
        "                    ('text_preprocessor', TextPreprocessor()),\n",
        "                    ('vectorizer', vectorizer),\n",
        "                    ('classifier', model_queue)\n",
        "                ])\n",
        "\n",
        "                pipeline_priority = PipelineModules(steps=[\n",
        "                    ('email_preprocessor', EmailPreprocessor()),\n",
        "                    ('text_preprocessor', TextPreprocessor()),\n",
        "                    ('vectorizer', vectorizer),\n",
        "                    ('classifier', model_priority)\n",
        "                ])\n",
        "\n",
        "        elif email.model_choice.lower() == 'lr':\n",
        "            if LOAD_MODE == 'pipeline':\n",
        "                lr_pipeline_queue_path = os.getenv('LR_PIPELINE_QUEUE_PATH')\n",
        "                lr_pipeline_priority_path = os.getenv('LR_PIPELINE_PRIORITY_PATH')\n",
        "\n",
        "                pipeline_queue = PipelineModules(\n",
        "                    from_file=True,\n",
        "                    file_path=lr_pipeline_queue_path\n",
        "                )\n",
        "\n",
        "                pipeline_priority = PipelineModules(\n",
        "                    from_file=True,\n",
        "                    file_path=lr_pipeline_priority_path\n",
        "                )\n",
        "            elif LOAD_MODE == 'model':\n",
        "                lr_model_queue_path = os.getenv('LR_MODEL_QUEUE_PATH')\n",
        "                lr_model_priority_path = os.getenv('LR_MODEL_PRIORITY_PATH')\n",
        "\n",
        "                model_queue = BaselineModel(model_path=lr_model_queue_path)\n",
        "                model_priority = BaselineModel(model_path=lr_model_priority_path)\n",
        "\n",
        "                pipeline_queue = PipelineModules(steps=[\n",
        "                    ('email_preprocessor', EmailPreprocessor()),\n",
        "                    ('text_preprocessor', TextPreprocessor()),\n",
        "                    ('vectorizer', vectorizer),\n",
        "                    ('classifier', model_queue)\n",
        "                ])\n",
        "\n",
        "                pipeline_priority = PipelineModules(steps=[\n",
        "                    ('email_preprocessor', EmailPreprocessor()),\n",
        "                    ('text_preprocessor', TextPreprocessor()),\n",
        "                    ('vectorizer', vectorizer),\n",
        "                    ('classifier', model_priority)\n",
        "                ])\n",
        "\n",
        "        if pipeline_queue.steps is None or pipeline_priority.steps is None:\n",
        "            raise HTTPException(status_code=400, detail=\"Pipeline not found\")\n",
        "\n",
        "        if FIT_FLG:\n",
        "            pipeline_queue = PipelineModules(steps=[\n",
        "                ('email_preprocessor', EmailPreprocessor()),\n",
        "                ('text_preprocessor', TextPreprocessor()),\n",
        "                ('label_preprocessor', LabelPreprocessor(label_column_name='queue')),\n",
        "                ('vectorizer', VectorizerPreprocessor()),\n",
        "                ('classifier', BaselineModel(model='LogisticRegression')) if email.model_choice.lower() == 'lr' else BaselineModel(model='MultinomialNB')\n",
        "            ])\n",
        "            pipeline_queue.fit(data_prep_train_df)\n",
        "\n",
        "            pipeline_priority = PipelineModules(steps=[\n",
        "                ('email_preprocessor', EmailPreprocessor()),\n",
        "                ('text_preprocessor', TextPreprocessor()),\n",
        "                ('label_preprocessor', LabelPreprocessor(label_column_name='priority')),\n",
        "                ('vectorizer', VectorizerPreprocessor()),\n",
        "                ('classifier', BaselineModel(model='LogisticRegression')) if email.model_choice.lower() == 'lr' else BaselineModel(model='MultinomialNB')\n",
        "            ])\n",
        "            pipeline_priority.fit(data_prep_train_df)\n",
        "\n",
        "    elif \"bert\" in email.model_choice.lower():\n",
        "        device = get_available_device()\n",
        "\n",
        "        if email.model_choice.lower() == 'bert':\n",
        "            if LOAD_MODE == 'pipeline':\n",
        "                bert_pipeline_queue_path = os.getenv('BERT_PIPELINE_QUEUE_PATH')\n",
        "                bert_pipeline_priority_path = os.getenv('BERT_PIPELINE_PRIORITY_PATH')\n",
        "\n",
        "                pipeline_queue = PipelineModules(\n",
        "                    from_file=True,\n",
        "                    file_path=bert_pipeline_queue_path,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                pipeline_priority = PipelineModules(\n",
        "                    from_file=True,\n",
        "                    file_path=bert_pipeline_priority_path,\n",
        "                    device=device\n",
        "                )\n",
        "            elif LOAD_MODE == 'model':\n",
        "                bert_model_queue_path = os.getenv('BERT_TRANSFORMERS_MODEL_QUEUE_PATH')\n",
        "                bert_model_priority_path = os.getenv('BERT_TRANSFORMERS_MODEL_PRIORITY_PATH')\n",
        "\n",
        "                model_queue = TransformerModel(from_filte=True, load_path=bert_model_queue_path, device=device)\n",
        "                model_priority = TransformerModel(from_file=True, load_path=bert_model_priority_path, device=device)\n",
        "\n",
        "                pipeline_queue = PipelineModules(steps=[\n",
        "                    ('email_preprocessor', EmailPreprocessor()),\n",
        "                    ('text_preprocessor', TextPreprocessor()),\n",
        "                    ('classifier', model_queue)\n",
        "                ])\n",
        "\n",
        "                pipeline_priority = PipelineModules(steps=[\n",
        "                    ('email_preprocessor', EmailPreprocessor()),\n",
        "                    ('text_preprocessor', TextPreprocessor()),\n",
        "                    ('classifier', model_priority)\n",
        "                ])\n",
        "\n",
        "        elif email.model_choice.lower() == 'distilbert':\n",
        "            if LOAD_MODE == 'pipeline':\n",
        "                distilbert_pipeline_queue_path = os.getenv('DISTILBERT_PIPELINE_QUEUE_PATH')\n",
        "                distilbert_pipeline_priority_path = os.getenv('DISTILBERT_PIPELINE_PRIORITY_PATH')\n",
        "\n",
        "                pipeline_queue = PipelineModules(\n",
        "                    from_file=True,\n",
        "                    file_path=distilbert_pipeline_queue_path,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                pipeline_priority = PipelineModules(\n",
        "                    from_file=True,\n",
        "                    file_path=distilbert_pipeline_priority_path,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "            elif LOAD_MODE == 'model':\n",
        "                distilbert_model_queue_path = os.getenv('DISTILBERT_TRANSFORMERS_MODEL_QUEUE_PATH')\n",
        "                distilbert_model_priority_path = os.getenv('DISTILBERT_TRANSFORMERS_MODEL_PRIORITY_PATH')\n",
        "\n",
        "                model_queue = TransformerModel(model_path=distilbert_model_queue_path, device=device)\n",
        "                model_priority = TransformerModel(model_path=distilbert_model_priority_path, device=device)\n",
        "\n",
        "                pipeline_queue = PipelineModules(steps=[\n",
        "                    ('email_preprocessor', EmailPreprocessor()),\n",
        "                    ('text_preprocessor', TextPreprocessor()),\n",
        "                    ('classifier', model_queue)\n",
        "                ])\n",
        "\n",
        "                pipeline_priority = PipelineModules(steps=[\n",
        "                    ('email_preprocessor', EmailPreprocessor()),\n",
        "                    ('text_preprocessor', TextPreprocessor()),\n",
        "                    ('classifier', model_priority)\n",
        "                ])\n",
        "\n",
        "        if FIT_FLG:\n",
        "            pipeline_queue = PipelineModules(steps=[\n",
        "                ('email_preprocessor', EmailPreprocessor()),\n",
        "                ('text_preprocessor', TextPreprocessor()),\n",
        "                ('label_preprocessor', LabelPreprocessor(label_column_name='queue')),\n",
        "                ('classifier', TransformerModel(model='bert-base-uncased', device=device)) if email.model_choice.lower() == 'bert' else TransformerModel(model='distilbert-base-uncased', device=device)\n",
        "            ])\n",
        "            pipeline_queue.fit(data_prep_train_df)\n",
        "\n",
        "            pipeline_priority = PipelineModules(steps=[\n",
        "                ('email_preprocessor', EmailPreprocessor()),\n",
        "                ('text_preprocessor', TextPreprocessor()),\n",
        "                ('label_preprocessor', LabelPreprocessor(label_column_name='priority')),\n",
        "                ('classifier', TransformerModel(model='bert-base-uncased', device=device)) if email.model_choice.lower() == 'bert' else TransformerModel(model='distilbert-base-uncased', device=device)\n",
        "            ])\n",
        "            pipeline_priority.fit(data_prep_train_df)\n",
        "\n",
        "    else:\n",
        "        raise HTTPException(status_code=400, detail=\"Invalid model choice\")\n",
        "\n",
        "    df_prep_email = df_email\n",
        "    df_prep_email = pipeline_queue.transform(df_prep_email)\n",
        "    df_prep_email = pipeline_queue.predict(df_prep_email)\n",
        "    pred_queue = df_prep_email['prediction'].values[0]\n",
        "    df_prep_email = pipeline_queue.predict_proba(df_prep_email)\n",
        "    pred_queue_proba = df_prep_email['prediction_proba'].values[0]\n",
        "\n",
        "    df_prep_email = df_email\n",
        "    df_prep_email = pipeline_priority.transform(df_prep_email)\n",
        "    df_prep_email = pipeline_priority.predict(df_prep_email)\n",
        "    pred_priority = df_prep_email['prediction'].values[0]\n",
        "    df_prep_email = pipeline_priority.predict_proba(df_prep_email)\n",
        "    pred_priority_proba = df_prep_email['prediction_proba'].values[0]\n",
        "\n",
        "    details = {\n",
        "        \"model\": email.model_choice,\n",
        "        \"dataframe_shape\": df_prep_email.shape,\n",
        "        \"predict_proba_queue\": pred_queue_proba.tolist(),\n",
        "        \"predict_proba_priority\": pred_priority_proba.tolist()\n",
        "    }\n",
        "\n",
        "    pred_queue_name = label_queue_values[pred_queue]\n",
        "    pred_priority_name = label_priority_values[pred_priority]\n",
        "\n",
        "    return PredictionResponse(queue=pred_queue_name, priority=pred_priority_name, details=details)"
      ],
      "metadata": {
        "id": "paV6RwyyoZpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !fastapi dev main.py"
      ],
      "metadata": {
        "id": "nnQ-ei4x0c5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_payload = {\n",
        "    \"subject\": \"Meeting Reminder\",\n",
        "    \"body\": \"Hello, the meeting is at 3 PM. Confirm attendance please.\",\n",
        "    \"model_choice\": \"nb\"\n",
        "}\n",
        "object_payload = EmailInput(**dict_payload)\n",
        "\n",
        "response = predict(object_payload)\n",
        "\n",
        "response"
      ],
      "metadata": {
        "id": "4HYGDu9A5Wh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UI"
      ],
      "metadata": {
        "id": "lWrBNHq1aCNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File gradio_app.py"
      ],
      "metadata": {
        "id": "SpWRXgq7aGZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "import requests\n",
        "# from src.utils.pydantic_models import EmailInput, PredictionResponse\n",
        "\n",
        "# URL of the locally running API server\n",
        "API_URL = os.getenv(\"API_URL\", \"http://localhost:8000/predict\")\n",
        "\n",
        "def predict_email(subject, body, model_choice):\n",
        "    payload = {\"subject\": subject, \"body\": body, \"model_choice\": model_choice}\n",
        "    response = requests.post(API_URL, json=payload)\n",
        "    # object_payload = EmailInput(**dict_payload)\n",
        "    # response = predict(object_payload)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return f\"Queue: {data['queue']}\\nPriority: {data['priority']}\\nDetails: {data['details']}\"\n",
        "    else:\n",
        "        return f\"Error: {response.text}\"\n",
        "\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict_email,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Subject\"),\n",
        "        gr.Textbox(label=\"Body\", lines=4),\n",
        "        gr.Radio(choices=[\"nb\", \"lr\", \"distilbert\", \"bert\"], label=\"Model Choice\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Customer IT Support Email Classifier\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()"
      ],
      "metadata": {
        "id": "lN1AHK9kaE-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File streamlit_app.py"
      ],
      "metadata": {
        "id": "LFv3MV0udWTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import requests\n",
        "import os\n",
        "\n",
        "API_URL = os.getenv(\"API_URL\", \"http://localhost:8000/predict\")\n",
        "\n",
        "st.title(\"Customer IT Support Email Classifier\")  # Matching Gradio's title\n",
        "\n",
        "subject = st.text_input(\"Subject\")  # Simplified label\n",
        "body = st.text_area(\"Body\", height=150)  # Adjust height as needed\n",
        "model_choice = st.radio(\"Model Choice\", options=[\"nb\", \"lr\", \"distilbert\", \"bert\"])  # Using radio buttons\n",
        "\n",
        "if st.button(\"Classify\"):\n",
        "    payload = {\"subject\": subject, \"body\": body, \"model_choice\": model_choice}\n",
        "    response = requests.post(API_URL, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        st.write(f\"Queue: {data['queue']}\\nPriority: {data['priority']}\\nDetails: {data['details']}\")  # Displaying all details\n",
        "    else:\n",
        "        st.error(f\"Error: {response.text}\")  # Displaying error details"
      ],
      "metadata": {
        "id": "Q0upzjzWdcJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py"
      ],
      "metadata": {
        "id": "7ThHEbmsmOQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage Examples"
      ],
      "metadata": {
        "id": "wb-2KmOiZafx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_prep_filter_queue = Pipeline(steps=[\n",
        "    ('email_preprocessor', EmailPreprocessor()),\n",
        "    ('resampling_preprocessor', ResamplingPreprocessor(label_columns='queue', resample_mode=None))\n",
        "])\n",
        "\n",
        "pipe_prep_split_queue = Pipeline(steps=[\n",
        "    ('splitter', SplitterPreprocessor())\n",
        "])\n",
        "\n",
        "pipe_prep_text_queue = Pipeline(steps=[\n",
        "    ('text_preprocessor', TextPreprocessor())\n",
        "])\n",
        "\n",
        "pipe_prep_vectorize_text_queue = Pipeline(steps=[\n",
        "    ('vectorizer', VectorizerPreprocessor())\n",
        "])\n",
        "\n",
        "pipe_prep_label_queue = Pipeline(steps=[\n",
        "    ('label_preprocessor', LabelPreprocessor(label_column_name='queue'))\n",
        "])"
      ],
      "metadata": {
        "id": "OTVVbB4ogVfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from data_examples import email_examples\n",
        "\n",
        "data_df = pd.DataFrame(email_examples)\n",
        "\n",
        "data_df"
      ],
      "metadata": {
        "id": "6k-XL93Vy98V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep_df = data_df.copy()\n",
        "\n",
        "data_prep_train_df, data_prep_val_df, data_prep_text_df = pipe_prep_split_queue.fit_transform(data_prep_df)\n",
        "\n",
        "print('Train Dataset:')\n",
        "display(data_prep_train_df.head())\n",
        "\n",
        "print('Validation Dataset:')\n",
        "display(data_prep_val_df.head())\n",
        "\n",
        "print('Test Dataset:')\n",
        "display(data_prep_text_df.head())"
      ],
      "metadata": {
        "id": "I-184mk_SMfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep_train_df = pipe_prep_filter_queue.fit_transform(data_prep_train_df)\n",
        "\n",
        "data_prep_train_df = pipe_prep_text_queue.fit_transform(data_prep_train_df)\n",
        "\n",
        "data_prep_train_df = pipe_prep_label_queue.fit_transform(data_prep_train_df)\n",
        "\n",
        "data_prep_train_df = pipe_prep_vectorize_text_queue.fit_transform(data_prep_train_df)\n",
        "\n",
        "data_prep_train_df"
      ],
      "metadata": {
        "id": "bVjU1bN4Nq5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_lr_queue = PipelineModules(steps=[\n",
        "    ('email_preprocessor', EmailPreprocessor()),\n",
        "    ('resampling_preprocessor', ResamplingPreprocessor(label_columns='queue', resample_mode=None)),\n",
        "    ('text_preprocessor', TextPreprocessor()),\n",
        "    ('vectorizer', VectorizerPreprocessor()),\n",
        "    ('label_preprocessor', LabelPreprocessor(label_column_name='queue')),\n",
        "    ('classifier', BaselineModel(model='LogisticRegression'))\n",
        "])\n",
        "\n",
        "display(pipeline_lr_queue)\n",
        "\n",
        "splitter = SplitterPreprocessor(retrieve='all')\n",
        "\n",
        "data_prep_train_df, data_prep_val_df, data_prep_text_df = splitter.fit_transform(data_prep_df)\n",
        "\n",
        "data_prep_train_df = pipeline_lr_queue.fit(data_prep_train_df)\n",
        "\n",
        "data_prep_train_df = pipeline_lr_queue.predict(data_prep_train_df)\n",
        "\n",
        "metrics = pipeline_lr_queue.evaluate(data_prep_train_df)\n",
        "\n",
        "print('Metrics on Train Dataset:', metrics)\n",
        "\n",
        "data_prep_val_df = pipeline_lr_queue.transform(data_prep_val_df)\n",
        "\n",
        "data_prep_val_df = pipeline_lr_queue.predict(data_prep_val_df)\n",
        "\n",
        "print('Metrics on Validation Dataset:', metrics)"
      ],
      "metadata": {
        "id": "lyzrBQWd0Yew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_nb_queue = PipelineModules(steps=[\n",
        "    ('email_preprocessor', EmailPreprocessor()),\n",
        "    ('resampling_preprocessor', ResamplingPreprocessor(label_columns='queue', resample_mode=None)),\n",
        "    ('text_preprocessor', TextPreprocessor()),\n",
        "    ('vectorizer', VectorizerPreprocessor()),\n",
        "    ('label_preprocessor', LabelPreprocessor(label_column_name='queue')),\n",
        "    ('classifier', BaselineModel(model='MultinomialNB'))\n",
        "])\n",
        "\n",
        "display(pipeline_nb_queue)\n",
        "\n",
        "splitter = SplitterPreprocessor(retrieve='all')\n",
        "\n",
        "data_prep_train_df, data_prep_val_df, data_prep_text_df = splitter.fit_transform(data_prep_df)\n",
        "\n",
        "data_prep_train_df = pipeline_nb_queue.fit(data_prep_train_df)\n",
        "\n",
        "data_prep_train_df = pipeline_nb_queue.predict(data_prep_train_df)\n",
        "\n",
        "metrics = pipeline_nb_queue\n",
        "\n",
        "print('Metrics on Train Dataset:', metrics)\n",
        "\n",
        "data_prep_val_df = pipeline_nb_queue.transform(data_prep_val_df)\n",
        "\n",
        "data_prep_val_df = pipeline_nb_queue.predict(data_prep_val_df)\n",
        "\n",
        "print('Metrics on Validation Dataset:', metrics)"
      ],
      "metadata": {
        "id": "IwG4UWn0zKnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Poetry"
      ],
      "metadata": {
        "id": "-CiWzO3uejk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File pyproject.toml"
      ],
      "metadata": {
        "id": "pZWYVPoZel3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    [tool.poetry]\n",
        "    name = \"customer_it_support\"\n",
        "    version = \"0.1.0\"\n",
        "    description = \"Email classification system for customer IT support using ML and Transformers\"\n",
        "    authors = [\"Your Name <your.email@example.com>\"]\n",
        "    license = \"MIT\"\n",
        "    readme = \"README.md\"\n",
        "    packages = [{include = \"src\"}]\n",
        "\n",
        "    [tool.poetry.dependencies]\n",
        "    python = \">=3.11.6,<3.12\"\n",
        "\n",
        "    # Backend (API & ML)\n",
        "    fastapi = \"^0.115.11\"\n",
        "    uvicorn = \"^0.34.0\"\n",
        "    torch = \"^2.6.0\"\n",
        "    transformers = \"^4.49.0\"\n",
        "    datasets = \"^3.3.2\"\n",
        "    scikit-learn = \"^1.6.1\"\n",
        "    nltk = \"^3.9.1\"\n",
        "    spacy = \"^3.8.4\"\n",
        "    wordcloud = \"^1.9.4\"\n",
        "    wandb = \"^0.19.8\"\n",
        "    accelerate = \"^1.4.0\"\n",
        "\n",
        "    # Data Processing & Visualization\n",
        "    pandas = \"^2.2.3\"\n",
        "    numpy = \"^2.2.3\"\n",
        "    matplotlib = \"^3.10.1\"\n",
        "    seaborn = \"^0.13.2\"\n",
        "\n",
        "    # UI Apps\n",
        "    gradio = \"^5.20.0\"\n",
        "    streamlit = \"^1.43.0\"\n",
        "\n",
        "    # Environment & Config\n",
        "    python-dotenv = \"^1.0.1\"\n",
        "    ipywidgets = \"^8.1.5\"\n",
        "\n",
        "    [tool.poetry.dev-dependencies]\n",
        "    pytest = \"^7.0\"\n",
        "    black = \"^24.0\"\n",
        "    flake8 = \"^6.0\"\n",
        "    mypy = \"^1.0\"\n",
        "\n",
        "    [tool.poetry.scripts]\n",
        "    start-api = \"uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload\"\n",
        "    start-gradio = \"python src/ui/gradio_app.py\"\n",
        "    start-streamlit = \"streamlit run src/ui/streamlit_app.py\"\n",
        "    start-all = \"bash run_all.sh\"\n",
        "\n",
        "    [build-system]\n",
        "    requires = [\"poetry-core\"]\n",
        "    build-backend = \"poetry.core.masonry.api\""
      ],
      "metadata": {
        "id": "P5FcQslGfFBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bash"
      ],
      "metadata": {
        "id": "772dftj9d5YF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File run_all.sh"
      ],
      "metadata": {
        "id": "o6_iInpGd_jO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    #!/bin/bash\n",
        "    poetry run uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload &\n",
        "    poetry run python src/ui/gradio_app.py &\n",
        "    poetry run streamlit run src/ui/streamlit_app.py"
      ],
      "metadata": {
        "id": "FuyMB36me2pV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    chmod +x run_all.sh"
      ],
      "metadata": {
        "id": "rmCMENkie6P2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    # poetry run start-api\n",
        "    # poetry run start-gradio\n",
        "    # poetry run start-streamlit\n",
        "    poetry run start-all"
      ],
      "metadata": {
        "id": "pcf7qlMKe-ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References\n",
        "\n",
        "- [Getting Started with scikit-learn Pipelines for Machine Learning](https://medium.com/analytics-vidhya/getting-started-with-scikit-learn-pipelines-for-machine-learning-fa88efdca3b9)"
      ],
      "metadata": {
        "id": "EQ03ZfkJsHXZ"
      }
    }
  ]
}